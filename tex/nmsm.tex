% !TEX program=lualatex
\documentclass{gulartcl}
\usepackage{preamble}

\title{NMSM Homework Exercises}
\author{Guglielmo Bordin}
\date{\today}

\begin{document}
\maketitle
\section{Sampling random points within \emph{d}-dimensional domains by hit and
miss}
I skipped the integration on the rectangle, solving only the disk case.  The
source code is in \texttt{A01b\_disk\_hit\_miss.c}; I implemented the main part
of the algorithm like this:
\lstinputlisting[
    firstline=32, lastline=46, language=C
]{../src/A01b_disk_hit_miss.c}
The error as a function of the number of throws is shown in \figref{fig:A01b}.
It is comfortably under \qty{1}{\percent} with around \numrange{25000}{30000}
iterations.

\begin{figure}
    \centering
    \includesvg[inkscapelatex=false]{img/A01b.svg}
    \caption{error in the Monte Carlo estimation of the area of a unit
        disk, as a function of the number of \textquote{throws}.}
    \label{fig:A01b}
\end{figure}

\section{Sampling random numbers from a given distribution}
The idea is to sample from the probability distribution $\rho_n(x) = c x^n$ in
$[0, 1]$.  First, using the normalization condition we can find out what $c$
should be:
\begin{equation}
    1 = \int_{0}^{1} cx^n \, dx = \frac{c}{n + 1} \implies c = n + 1.
\end{equation}
Then, we find the expression of the associated cumulative density function:
\begin{equation}
    F_n(x) = (n + 1) \int_{0}^{x} y^n \, dy = x^{n + 1},
\end{equation}
and invert it:
\begin{equation}
    u = x^{n + 1} \implies x = u^{1 / (n + 1)}.
\end{equation}

So, inside the code \texttt{A02a\_inversion\_method.c} I sample a random
\texttt{double} from a uniform distribution between $0$ and $1$ using
\texttt{drand48()}, and I raise it to the power of $1 / (n + 1)$ to get $x$:
\lstinputlisting[
    firstline=25, lastline=27, language=C
]{../src/A02a_inversion_method.c}
A histogram of \num{100000} points sampled from $\rho$ with $n = 3$ is displayed
in \figref{fig:A02a_3}.

\begin{figure}
    \centering
    \includesvg[inkscapelatex=false]{img/A02a_3.svg}
    \caption{histogram of \num{100000} points sampled from the probability
        distribution $4 x^3$ in $[0, 1]$.}
    \label{fig:A02a_3}
\end{figure}

Inside \texttt{A02b\_inversion\_method.c} I modified the code to sample from
$\rho_2(x) = cx^2$ in $[0, 2]$. $c$ is different this time, of course:

\begin{equation}
    1 = \int_{0}^{2} cx^2 \, dx = \frac{8}{3}c \implies c = \frac{3}{8}.
\end{equation}
The cumulative is then
\begin{equation}
    F_2(x) = \frac{3}{8} \int_{0}^{x} y^2 \, dy = \frac{x^3}{8} \implies x = 2
    u^{1/3}.
\end{equation}
Once again, you can see the comparison between a \num{100000}-points histogram
and the theoretical curve in \figref{fig:A02b}.

\begin{figure}
    \centering
    \includesvg[inkscapelatex=false]{img/A02b.svg}
    \caption{histogram of \num{100000} points sampled from the probability
        distribution $3x^2/8$ in $[0, 2]$.}
    \label{fig:A02b}
\end{figure}

\section{Sampling via transformation of coordinates}
If we want to sample points uniformly distributed over the unit disk, we cannot
simply generate $r \sim \unif{0}{1}$ and $\theta \sim \unif{0}{2\pi}$. The
result of doing that is shown in \figref{subfig:A03aa}, and the corresponding
source code is \texttt{A03aa\_disk\_naive.c}. While the angular distribution
poses no problem, there is an undesired higher density of points close to the
centre of the disk.

\begin{figure}
    \centering
    \begin{minipage}[t]{0.5\linewidth}
        \centering
        \includesvg[inkscapelatex=false]{img/A03aa.svg}
        \subcaption{}
        \label{subfig:A03aa}
    \end{minipage}\hfill%
    \begin{minipage}[t]{0.5\linewidth} 
        \centering
        \includesvg[inkscapelatex=false]{img/A03ab.svg}    
        \subcaption{}
        \label{subfig:A03ab}
    \end{minipage}
    \caption{\num{10000} points sampled on the unit disk with the wrong
        coordinate transformation (a) and with the correct one (b).}
    \label{fig:A03a}
\end{figure}

The reason for this becomes clear when you consider the number of points falling
inside a ring of given thickness $\delta r$. Consider a ring with inner radius
$r_1$ and outer radius $r_1 + \delta r$, with $\delta r \ll r_1$, and another
with radii $r_2$, $r_2 + \delta r$.  Despite the ratio of their areas being $r_2
/ r_1$, the fraction of points within each ring remains the same. Thus, the
number of points falling at a distance $r$ from the centre should be
proportional to $r$:
\begin{equation}
    \rho_{r}(r) = 2r,
\end{equation}
with the $2$ in front to ensure normalization over $[0, 1]$. By computing the
cumulative and inverting we get then
\begin{equation}
    F_r(r) = 2 \int_{0}^{x} x \, dx = x^{2} \implies u = r^{2} \implies r =
    \sqrt{u}.
\end{equation}

So, I modified the code in \texttt{A03ab\_disk\_correct.c} to sample like this:
\lstinputlisting[
    firstline=25, lastline=32, language=C
]{../src/A03ab_disk_correct.c}
The result is the correctly uniform sampling in \figref{subfig:A03ab}.

As regards the Box–Muller transform, I started from the factorization of
$\rho_{x, y}(x, y)$:
\begin{equation}
    \rho_{x, y}(x, y) = \frac{1}{2\pi} \exp\left(-\frac{x^{2} + y^{2}}{2}\right)
        = \rho_x(x) \rho_y(y), \quad \text{with }
        \rho_x(x) = \frac{e^{-x^{2}/2}}{\sqrt{2\pi}}.
\end{equation}
If we switch to polar coordinates this becomes, remembering the factor $r$
coming from the Jacobian,
\begin{equation}
    \rho_{r, \theta}(r, \theta) = \frac{r}{2\pi} e^{-r^{2}/2},
\end{equation}
with $\rho_r(r) = r e^{-r^{2}/2}$ and $\rho_\theta(\theta) = 1 / 2\pi$.

Then, we can marginalize over $\theta$ to get $\rho_r(r)$:
\begin{equation}
    \rho_r(r) = \frac{1}{2\pi} \int_{0}^{2\pi} r e^{-r^{2}/2} \, dr
        = r e^{-r^{2}/2}.
\end{equation}
As usual, we compute the cumulative and invert it to generate a random $r \sim
\rho_r(r)$:
\begin{equation}
    F_r(r) = \int_{0}^{r} x e^{-x^{2}/2} \, dx = 1 - e^{-r^{2}/2}
    \implies e^{-r^{2}/2} = 1 - u.
\end{equation}
Since $u$ is distributed uniformly, we can redefine it as $1 - u$ for
simplicity:
\begin{equation}
    -\frac{r^{2}}{2} = \log u \implies r = \sqrt{-2\log u}.
\end{equation}
Then, to get the angle $\theta$ we recover the \emph{conditional} distribution
$\rho_{\theta\given r}(\theta \given r)$:
\begin{equation}
    \rho_{\theta\given r}(\theta\given r)
        = \frac{\rho_{r, \theta}(r, \theta)}{\rho_{r}(r)} = \frac{1}{2\pi}.
\end{equation}
Thus to get $\theta$ we have simply to sample from $\unif{0}{2\pi}$.

The idea then is to sample two numbers $u_{1}, u_{2}$ from $\unif{0}{1}$ at each
iteration; at that point we can calculate
\begin{equation}
    x = \sqrt{-2\log u_1} \cos(2\pi u_2),
    \quad y = \sqrt{-2\log u_1} \sin(2\pi u_2),
\end{equation}
to get two numbers $x, y$ distributed according to a standard Gaussian
$\gaus{0}{1}$. We can also get a $z \sim \gaus{\mu}{\sigma}$ afterwards by
multiplying $x$ or $y$ by $\sigma$ and summing the mean: $z = \mu + \sigma x$.

The code is in \texttt{A03b\_box\_muller.c}; the relevant section is
\lstinputlisting[
    firstline=24, lastline=32, language=C,
]{../src/A03b_box_muller.c}
You can see a histogram of \num{100000} sampled points in \figref{fig:A03b}.

\begin{figure}
    \centering
    \includesvg[inkscapelatex=false]{img/A03b.svg}
    \caption{histogram of \num{100000} points sampled with the Box–Muller
        algorithm from the normal distribution with $\mu = 2$ and $\sigma = 3$.}
    \label{fig:A03b}
\end{figure}

\section{Rejection method}
In \texttt{A03ca\_rejection\_sampling.c} I implemented the sampling of random
numbers from the probability distribution
\begin{equation}
    f(x) = \frac{2}{\sqrt{pi}} e^{-x^{2}}
\end{equation}
over $[0, \infty)$. I started from a function $g(x)$ that could serve as an
upper limit to $f(x)$,
\begin{equation}
    g(x) =
    \begin{dcases}
        A & \text{for } 0 \leq x \leq p, \\
        \frac{A}{p}x e^{p^{2} - x^{2}} & \text{for } x > p.
    \end{dcases}
\end{equation}
First we normalize it to turn it into a proper probability distribution:
\begin{equation}
    1 = \int_{0}^{\infty} g(x) \, dx = Ap + \frac{A}{p}e^{p^{2}}
    \int_{p}^{\infty} x e^{-x^{2}} \, dx = A \left(p + \frac{1}{2p}\right)
    \implies A = \frac{2p}{1 + 2p^{2}}.
\end{equation}
Then we need a constant $c$ such that $c g(x) \geq f(x)$ everywhere. The maximum
of $f(x)$ is in $x = 0$, where $f(0) = 2 / \sqrt{\pi}$, so we set $c A = 2 /
\sqrt{\pi}$. Another thing to consider is that $g(x)$ can have a maximum larger
than $A$ in $[p, \infty)$:
\begin{equation}
    g'(x) = 
    \begin{dcases}
        0 & \text{for } 0 \leq x \leq p, \\
        \frac{A}{p}(1 - 2x^{2}) e^{p^{2} - x^{2}} & \text{for } x > p.
    \end{dcases}
\end{equation}
Thus in the region $[p, \infty)$ the derivative is zero in $x = 1 / \sqrt{2}$.
To avoid this we have to choose $p > 1 / \sqrt{2} \approx \num{0.707}$.

Now, to generate numbers distributed according to $g(x)$ we compute, as usual,
the cumulative and invert it: 
\begin{equation}
    G(x) =
    \begin{dcases}
        Ax & \text{for } 0 \leq x \leq p, \\
        1 - \frac{A}{2p} e^{p^{2} - x^{2}} & \text{for } x > p.
    \end{dcases}
\end{equation}
Since it is piecewise defined, we have to pay attention to the limits too:
\begin{gather}
    \begin{cases}
        x = u / A & \text{for } u \leq Ap, \\
        u = 1 - \frac{A}{2p} e^{p^{2} - x^{2}} & \text{for } u > Ap,
    \end{cases} \\
\shortintertext{which implies}
    x = 
    \begin{dcases}
        u / A & \text{for } u \leq Ap, \\
        \sqrt{p^{2} - \log\left[\frac{2p}{A}(1 - u)\right]} & \text{for } u > Ap.
    \end{dcases}
\end{gather}

With the choice $cA = 2 / \sqrt{\pi}$ the test $c g(x) \xi < f(x)$, with $\xi
\sim \unif{0}{1}$, becomes
\begin{equation}
    \cancel{\frac{2}{\sqrt{\pi}}} \xi < \cancel{\frac{2}{\sqrt{\pi}}}
    e^{-x^{2}} \implies \xi < e^{-x^{2}},
\end{equation}
if we generated $x$ with the uniform part of $g(x)$, or
\begin{equation}
    \xi \cancel{\frac{2}{\sqrt{\pi}}} \frac{x}{p}e^{p^{2} - x^{2}} <
    \cancel{\frac{2}{\sqrt{\pi}}} e^{-x^{2}} \implies \xi x < p e^{-p^{2}}
\end{equation}
otherwise. Let’s take a look at the main loop in the code to make it clear:
\lstinputlisting[
    firstline=30, lastline=44, language=C,
]{../src/A03ca_rejection_sampling.c}
As you can see from the example histogram in \figref{fig:A03ca}, the sampling is
correct.

\begin{figure}
    \centering
    \includesvg[inkscapelatex=false]{img/A03ca.svg}
    \caption{histogram of \num{100000} points sampled from $f(x) = 2 e^{-x^{2}}
        / \sqrt{\pi}$ with the rejection method implemented in
        \texttt{A03ca\_rejection\_sampling.c}.}
    \label{fig:A03ca}
\end{figure}

In \texttt{A03cb\_rejection\_analysis.c} I modified slightly the code in order
to analyse the acceptance rate of the algorithm as a function of $p$ – subject
to the condition I was mentioning earlier, $p > 1 / \sqrt{2}$. You can see the
results in \figref{fig:A03cb}: the best performance, unsurprisingly, is obtained
with $p = 1 / \sqrt{2}$, since for larger values of $p$ $g(x)$ tends to move
further away from $f(x)$.

\begin{figure}
    \centering
    \includesvg[inkscapelatex=false]{img/A03cb.svg}
    \caption{acceptance rate of \num{100000} samples generated with the
        rejection method algorithm in \texttt{A03cb\_rejection\_analysis.c}., as
        a function of the breakpoint of $g(x)$.}
    \label{fig:A03cb}
\end{figure}

\section{Importance sampling}
The goal was to integrate the function $f(x) = g(x) e^{-x^{2}}$ on $[0,
\infty)$, with $g(x)$ being a slowly varying function. I chose $g(x) = x
\cos x^{2}$, so the exact value of the integral is
\begin{equation}
    \int_{0}^{\infty} f(x) \, dx = \int_{0}^{\infty} x \cos(x^2) e^{-x^{2}}\, dx
        = \dots = \frac{1}{4}.
\end{equation}
Following the suggestion, we multiply and divide by a weighting function defined
as
\begin{equation}
    w(x) = \frac{2}{\sqrt{\pi}} e^{-x^{2}},
\end{equation}
and so we can write
\begin{equation}
    \int_{0}^{\infty} g(x) e^{-x^{2}} \, dx = \frac{\sqrt{\pi}}{2} \int_{0}^{\infty} 
        g(x) w(x) \, dx = \frac{\sqrt{\pi}}{2} \int_{0}^{\infty} g(x) \, dw(x).
\end{equation}
Therefore, we can sample numbers $x_i$ from $w(x)$ and estimate the integral
with
\begin{equation}
    \int_{0}^{\infty} g(x) e^{-x^{2}} \, dx \approx \frac{\sqrt{\pi}}{2N}
        \sum_{i = 1}^{N} g(x_i).
\end{equation}

To sample from $w(x)$, which is a half-normal distribution with variance $1/2$,
we can use a modified Box-Muller. The idea is to restrict the number generation
to the $x > 0$ half-plane. First, since in this case the variance is not unitary
we can recompute the cumulative. The marginal distribution over $r$ needs a
factor \num{2} in front for normalization, and we obtain
\begin{equation}
    \rho_r(r) = 2re^{-r^{2}} \implies F_r(r) = 1 - e^{-r^{2}} \implies
    r = \sqrt{-\log u_1}, \quad\text{with } u_1 \sim \unif{0}{1}.
\end{equation}
For $\theta$ we sample uniformly over $[-\pi/2, +\pi/2]$ in order to stay in the
$x > 0$ half-plane:
\begin{equation}
    \theta = -\frac{\pi}{2} + \pi u_2, \quad
    \text{with } u_2 \sim \unif{0}{1}.
\end{equation}
Once we have $r$ and $\theta$ we generate $x$, $y$ distributed according to
$w(x)$ in this way:
\begin{equation}
    x = r \cos\theta,\quad y = r\abs{\sin\theta}.
\end{equation}
Here is the function that implements this algorithm; the full source code is in
the file \texttt{A04a\_gauss\_crude\_vs\_import.c}.
\lstinputlisting[
    firstline=33, lastline=47, language=C,
]{../src/A04a_gauss_crude_vs_import.c}

In contrast to the importance sampling technique, inside the same C file I also
implemented the \textquote{crude} Monte Carlo integration, that is a simple
average of a set of $f(x_i)$ with $x_i$ sampled uniformly over an interval from
$0$ to a given $x_{\mathrm{max}}$:
\begin{equation}
    \int_{0}^{\infty} g(x) e^{-x^{2}}\, dx \approx
    = \frac{1}{N} \sum_{i = 1}^{N} g(x_i) e^{-x_i^2}, \quad
    \text{with } x_i \sim \unif{0}{x_{\mathrm{max}}}.
\end{equation}
The implementation is in this simple function:
\lstinputlisting[
    firstline=24, lastline=31, language=C,
]{../src/A04a_gauss_crude_vs_import.c}

I have compared the error of the two methods as a function of the number of
iterations in \figref{fig:A04a}. The importance sampling is definitely
more efficient, although the crude method has a higher variance which can lead
to a lower error in lucky runs. Also, as $N$ grows the difference between the
two methods shrinks, but this is rather unsurprising.

\begin{figure}
    \centering
    \includesvg[inkscapelatex=false]{img/A04a.svg}
    \caption{error of the Monte Carlo integration of $x \cos(x^{2}) e^{-x^{2}}$
        on $[0, \infty)$ as a function of the number of iterations, both with
        the \textquote{crude} method and the importance sampling. The error bars
        are obtained from the standard deviation of \num{100} different runs.}
    \label{fig:A04a}
\end{figure}

The second exercise asked to compute the integral
\begin{equation}
    \int_{0}^{\pi/2} \cos x \, dx
\end{equation}
using once again the importance sampling technique, with a weighting function
$g(x) = a + bx^2$, $a$ and $b$ to tune. To have the best performance, the idea
is to have a $g(x)$ that is large where the integrand is large and small when
the integrand is small. First, $g(x)$ should be normalized, which brings down the
number of parameters to one:
\begin{equation}
    1 = \int_{0}^{\pi/2} (a + bx^{2})\, dx = \frac{\pi}{2}a +
    \frac{\pi^{3}}{24}b \implies a = \frac{2}{\pi} - \frac{\pi^{2}}{12}b.
\end{equation}
Next, since we want $g(x)$ to be as similar as possible to $\cos x$, we can
restrict ourselves to $b < 0$. If $b$ is too negative, though, $g(x)$ becomes
negative in part of the domain. The best solution is to impose $g(\pi/2) = 0$:
\begin{equation}
    0 = \frac{2}{\pi} - \frac{\pi^{2}}{12}b + \frac{\pi^{2}}{4}b
    \implies b = -\frac{12}{\pi^{3}}
    \implies g(x) = \frac{3}{\pi}\left(1 - \frac{4}{\pi^{2}}x^{2}\right).
\end{equation}

To sample from $g$, we compute the cumulative as usual:
\begin{equation}
    G(x) = \frac{3}{\pi} \int_{0}^{x} \left(1 - \frac{4}{\pi^{2}}y^{2}\right)\,
    dy = \frac{3}{\pi}x - \frac{4}{\pi^{3}}x^{3}.
\end{equation}
The inversion requires solving a cubic equation,
\begin{equation}
    u = \frac{3}{\pi} x - \frac{4}{\pi^{3}}x^{3}
    \implies x^{3} - \frac{3}{4} \pi^{2}x + \frac{\pi^{3}}{4} u = 0.
\end{equation}
Let’s define $\alpha = \pi^{2}/4$:
\begin{equation}
    x^{3} - 3\alpha x + \pi \alpha u = 0.
\end{equation}
This is a depressed cubic, so thankfully it has a trigonometric solution given
by\footnote{\url{https://en.wikipedia.org/wiki/Cubic_equation\#Trigonometric_and_hyperbolic_solutions}.}
\begin{equation}
    x_k = 2 \sqrt{\alpha}
    \cos\left[\frac{1}{3}\arccos\left(-\frac{3\pi\cancel{\alpha}
    u}{6\cancel{\alpha}} \sqrt{\frac{\cancel{3}}{\cancel{3}\alpha}}\right) -
    \frac{2\pi}{3}k \right], \quad \text{with } k = 0, 1, 2.
\end{equation}
With some algebra, and selecting the right $k$, we can arrive at the much simpler form
\begin{equation}
    x = \pi \sin\left(\frac{1}{3} \arcsin u\right), \quad
    \text{with } u \sim \unif{0}{1}.
    \label{eq:x~a+bx2}
\end{equation}

To wrap up, what I ended up doing is
\begin{equation}
    \int_{0}^{\pi/2} \cos x \, dx \approx \frac{1}{N} \sum_{i = 1}^{N}
    \frac{\cos x_{i}}{g(x_{i})} = \frac{\pi}{3N} \sum_{i = 1}^{N} \frac{\cos
    x_{i}}{1 - 4 x_{i}^{2} / \pi^{2}},
\end{equation}
with each $x_i$ generated according to Eq.~\eqref{eq:x~a+bx2}. The code is in
\texttt{A04b\_cosx\_importance.c}, with the main function being
\lstinputlisting[
    firstline=14, lastline=25, language=C,
]{../src/A04b_cosx_importance.c}
The integration error as a function of the number of iterations is reported in
\figref{fig:A04b}. Remarkably, as low as \num{100} iterations are sufficient to
get on average an error that is comfortably under \qty{1}{\percent}.

\begin{figure}
    \centering
    \includesvg[inkscapelatex=false]{img/A04b.svg}
    \caption{error of the Monte Carlo integration with importance sampling of
        $\cos x$ on $[0, \pi/2]$ as a function of the number of iterations. The
        error bars are obtained from the standard deviation of \num{100}
        different runs.}
    \label{fig:A04b}
\end{figure}

The third exercise asked to investigate the improvement one can get with
importance sampling applied to a simple integral, the average on the
distribution $\rho(x) = e^{-x}$, $x \geq 0$, of the function
\begin{equation}
    f(x) =
    \begin{cases}
        0 &\quad \text{for } x < T, \\
        1 &\quad \text{for } x \geq T.
    \end{cases}
\end{equation}
This average can be readily calculated as
\begin{equation}
    \expval{f}_{\rho} = \int_{0}^{\infty} f(x) e^{-x} \, dx = \int_{T}^{\infty}
    e^{-x}\, dx = e^{-T}.
\end{equation}
The problem of this integral is that $f(x)$ is different from $0$ in the region
where $\rho(x)$ is quickly vanishing. This means that if we were to sample
numbers from $\rho(x)$ we would waste a lot of iterations.

Consider instead the function $g(x; a) = ae^{-ax}$ defined for $x \geq 0$ and $a
\in (0, 1]$. We can use it for an importance sampling and optimize $a$ to
minimize the variance. In particular, defining $F(x) \equiv f(x) \rho(x) / g(x)$
we can write
\begin{equation}
    \expval{f}_{\rho} = \int_{0}^{\infty} f(x) \rho(x) \, dx = 
    \int_{0}^{\infty} \frac{f(x)\rho(x)}{g(x)} g(x) \, dx = 
    \expval{F}_{g}.
\end{equation}
Let’s calculate the second moment now:
\begin{equation}
\begin{split}
    \expval{F^{2}}_{g} = \int_{0}^{\infty} \frac{f^{2}(x) \rho^2(x)}{g^{2}(x)}
    g(x) \, dx &= \int_{T}^{\infty} \frac{\rho^{2}(x)}{g(x)}\, dx \\
    &= \frac{1}{a} \int_{T}^{\infty} e^{-(2-a) x} \, dx =
    \frac{e^{-(2-a)T}}{a(2-a)}.
\end{split}
\end{equation}
By subtracting the first moment squared we get the variance,
\begin{equation}
    \sigma^{2}_g(F) = \expval{F^{2}}_g - \expval{F}_{g}^{2}
    = \frac{e^{-(2-a)T}}{a(2-a)} - e^{-2T}.
\end{equation}
The variance of $f$ with respect to $\rho$ is given instead by
\begin{equation}
\begin{split}
    \sigma^{2}_{\rho}(f)
    = \int_{0}^{\infty} (f(x) - \expval{f}_{\rho})^{2} \rho(x) \, dx
    &= \int_{T}^{\infty} \rho(x) \, dx + \expval{f}_{\rho}^{2}
        - 2 \expval{f}_{\rho} \int_{T}^{\infty} \rho(x) \, dx \\
    &= \expval{f}_{\rho} + \expval{f}^{2}_{\rho} - \expval{f}^{2}_{\rho} \\
    &= e^{-T}(1 - e^{-T}).
\end{split}
\end{equation}

Now, let’s minimize $\sigma^{2}_g(F)$ with respect to $a$:
\begin{equation}
    \pdiff{}{a} \sigma^{2}_{g}(F) = e^{-(2-a)T} \left[\frac{Ta(2-a) -
    2(1-a)}{a^{2}(2-a)^{2}}\right],
\end{equation}
which gets us to
\begin{equation}
    Ta^{2} - 2(1 + T) a + 2 = 0 \iff a = a^*_{\pm}
    = \frac{1 + T \pm \sqrt{1 + T^{2}}}{T}.
\end{equation}
Only $a^*_{-}$ is compatible with the condition $a \in (0, 1]$, so the optimal
$a$ is given by
\begin{equation}
    a^* = \frac{1 + T - \sqrt{1 + T^{2}}}{T}.
\end{equation}

In Tab.~\ref{tab:A04c} I calculated the ratios between each of the two variances
and $\expval{f}_{\rho}$ and the ratio between the two variances, for some values
of $T$. While $\sigma_g(F) / \expval{f}_{\rho}$ grows slowly, $\sigma_\rho(f) /
\expval{f}_{\rho}$ has an exponential increase; indeed
\begin{equation}
    \frac{\sigma_{\rho}(f)}{\expval{f}_{\rho}} = \frac{\sqrt{e^{-T}(1 -
    e^{-T})}}{e^{-T}} = \sqrt{e^{-T} - 1}.
\end{equation}
In \figref{fig:A04c} you can see the plot of the variance ratio as a function of
$T$.

\begin{table}
    \centering
    \caption{variance ratios for some values of $T$, all evaluated at $a^{*}$.}
    \label{tab:A04c}
    \begin{tabular}{S[table-format=2] S[table-format=5.2, group-separator={}]
                    S[table-format=1.2] S[table-format=4.2, group-separator={}]}
        \toprule
        {$T$} & {$\sigma_{\rho}(f) / \expval{f}_{\rho}$}
              & {$\sigma_{g}(F) / \expval{F}_{g}$}
              & {$\sigma_{\rho}(f) / \sigma_{g}(F)$} \\
        \midrule
        3  & 4.37  & 1.95 & 2.24 \\
        5  & 12.14 & 2.55 & 4.76 \\
        10 & 148.41 & 3.65 & 40.71 \\
        20 & 22026.47 & 5.18 & 4249.17 \\
        \bottomrule 
    \end{tabular}
\end{table}

\begin{figure}
    \centering
    \includesvg[inkscapelatex=false]{img/A04c.svg}
    \caption{ratio between the two variances, $\sigma_{\rho}(f) /
        \sigma_{g}(F)$, computed with $a^*$ as a function of $T$.}
    \label{fig:A04c}
\end{figure}
\end{document}
